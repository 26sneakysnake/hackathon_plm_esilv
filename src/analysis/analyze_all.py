"""
Script principal pour exÃ©cuter toutes les analyses
"""

import pandas as pd
import json
from pathlib import Path
import sys

# Ajouter le chemin parent pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from analysis.process_mining import ProcessMiner
from analysis.bottleneck_detector import BottleneckDetector
from analysis.wip_analyzer import WIPAnalyzer
from analysis.rework_tracker import ReworkTracker


def run_complete_analysis(event_log_path: str):
    """ExÃ©cute toutes les analyses et gÃ©nÃ¨re un rapport"""

    print("=" * 80)
    print("ðŸš€ MANUFACTURING OPERATIONS RADAR - ANALYSE COMPLÃˆTE")
    print("=" * 80)

    # Charger l'event log
    print("\nðŸ“‚ Chargement de l'event log...")
    event_log = pd.read_csv(event_log_path)
    print(f"âœ… Event log chargÃ©: {len(event_log)} Ã©vÃ©nements, {event_log['case_id'].nunique()} piÃ¨ces")

    # ====================
    # 1. PROCESS MINING
    # ====================
    print("\n" + "=" * 80)
    print("ðŸ“Š 1. PROCESS MINING")
    print("=" * 80)

    pm = ProcessMiner(event_log)

    overview = pm.get_process_overview()
    print(f"\nðŸ” Vue d'ensemble:")
    print(f"  â€¢ Nombre de piÃ¨ces: {overview['nombre_pieces']}")
    print(f"  â€¢ Nombre d'opÃ©rations: {overview['nombre_operations']}")
    print(f"  â€¢ Lead time moyen: {overview['lead_time_moyen']:.2f}h (Â±{overview['lead_time_std']:.2f}h)")
    print(f"  â€¢ Lead time min/max: {overview['lead_time_min']:.2f}h / {overview['lead_time_max']:.2f}h")
    print(f"  â€¢ Taux de rework: {overview['taux_rework']:.1f}%")
    print(f"  â€¢ DÃ©bit: {overview['throughput']:.3f} piÃ¨ces/heure")
    print(f"  â€¢ PÃ©riode: {overview['periode_debut']} â†’ {overview['periode_fin']}")

    print(f"\nðŸ“Š Temps de cycle par opÃ©ration:")
    cycle_times = pm.calculate_cycle_times()
    print(cycle_times[['Temps RÃ©el Moyen (h)', 'Temps Attente Moyen (h)', 'Nombre Ã‰vÃ©nements']].head(10))

    # ====================
    # 2. BOTTLENECK DETECTION
    # ====================
    print("\n" + "=" * 80)
    print("ðŸš¨ 2. DÃ‰TECTION DES GOULOTS D'Ã‰TRANGLEMENT")
    print("=" * 80)

    bd = BottleneckDetector(event_log)

    print("\nðŸ”´ Goulots par temps d'attente:")
    wait_bottlenecks = bd.detect_bottlenecks_by_wait_time()
    print(wait_bottlenecks[['activity', 'wait_time_mean', 'cycle_time_mean', 'wait_to_cycle_ratio', 'is_bottleneck']].head(5))

    print("\nðŸ”´ Goulots par WIP:")
    wip_bottlenecks = bd.detect_bottlenecks_by_wip()
    print(wip_bottlenecks[['activity', 'wip_mean', 'wip_max', 'is_bottleneck']].head(5))

    print("\nðŸ”´ Impact sur le lead time:")
    impact = bd.calculate_bottleneck_impact()
    print(impact[['activity', 'total_time', 'leadtime_contribution_pct']].head(5))

    # ====================
    # 3. WIP ANALYSIS
    # ====================
    print("\n" + "=" * 80)
    print("ðŸ“¦ 3. ANALYSE DU WIP (WORK IN PROGRESS)")
    print("=" * 80)

    wip = WIPAnalyzer(event_log)

    wip_by_activity = wip.calculate_wip_by_activity()
    print("\nðŸ“Š WIP par activitÃ©:")
    print(wip_by_activity[['activity', 'wip_mean', 'wip_max', 'wip_std']].head(8))

    inventory = wip.calculate_inventory_profile()
    print(f"\nðŸ“¦ Profil d'inventaire (Little's Law):")
    print(f"  â€¢ WIP thÃ©orique: {inventory['theoretical_wip']:.2f} piÃ¨ces")
    print(f"  â€¢ WIP rÃ©el moyen: {inventory['actual_wip']:.2f} piÃ¨ces")
    print(f"  â€¢ EfficacitÃ© WIP: {inventory['wip_efficiency']:.1f}%")

    flow_eff = wip.calculate_flow_efficiency()
    print(f"\nâš¡ EfficacitÃ© du flux:")
    print(f"  â€¢ Flow Efficiency moyenne: {flow_eff['avg_flow_efficiency']:.1f}%")
    print(f"  â€¢ Temps Ã  valeur ajoutÃ©e: {flow_eff['avg_value_adding_time']:.2f}h")
    print(f"  â€¢ Temps de gaspillage: {flow_eff['avg_waste_time']:.2f}h")

    accumulation = wip.identify_wip_accumulation_points()
    print(f"\nðŸš¨ Points d'accumulation de WIP ({len(accumulation)} trouvÃ©s):")
    if len(accumulation) > 0:
        print(accumulation[['activity', 'wip_mean', 'wip_excess', 'wip_excess_pct']].head(5))

    # ====================
    # 4. REWORK ANALYSIS
    # ====================
    print("\n" + "=" * 80)
    print("ðŸ”„ 4. ANALYSE DES REWORKS")
    print("=" * 80)

    rt = ReworkTracker(event_log)

    rework_rate = rt.calculate_rework_rate_by_activity()
    print("\nðŸ“Š Taux de rework par activitÃ©:")
    print(rework_rate[['activity', 'total_events', 'rework_events', 'rework_rate_pct']].head(8))

    rework_cost = rt.calculate_rework_cost()
    print(f"\nðŸ’° CoÃ»t des reworks:")
    if len(rework_cost) > 0:
        print(rework_cost[['activity', 'total_cost_euros', 'rework_count', 'total_time_hours']].head(5))
        print(f"\n  ðŸ’¸ CoÃ»t total des reworks: {rework_cost['total_cost_euros'].sum():.2f}â‚¬")

    leadtime_impact = rt.calculate_rework_impact_on_leadtime()
    print(f"\nâ±ï¸ Impact sur le lead time:")
    print(f"  â€¢ Lead time avec rework: {leadtime_impact['avg_leadtime_with_rework']:.2f}h")
    print(f"  â€¢ Lead time sans rework: {leadtime_impact['avg_leadtime_without_rework']:.2f}h")
    print(f"  â€¢ Augmentation: +{leadtime_impact['leadtime_increase_pct']:.1f}%")

    fpy = rt.calculate_first_pass_yield()
    print(f"\nâœ… First Pass Yield (FPY):")
    print(fpy[['activity', 'ok_count', 'total_count', 'fpy_pct']].head(8))

    # ====================
    # 5. RÃ‰SUMÃ‰ EXÃ‰CUTIF
    # ====================
    print("\n" + "=" * 80)
    print("ðŸ“‹ 5. RÃ‰SUMÃ‰ EXÃ‰CUTIF - KPIs CLÃ‰S")
    print("=" * 80)

    kpis = {
        'lead_time_moyen_h': round(overview['lead_time_moyen'], 2),
        'wip_moyen': round(inventory['actual_wip'], 2),
        'throughput_pieces_par_jour': round(overview['throughput'] * 24, 2),
        'taux_rework_pct': round(overview['taux_rework'], 1),
        'flow_efficiency_pct': round(flow_eff['avg_flow_efficiency'], 1),
        'cout_rework_total_euros': round(rework_cost['total_cost_euros'].sum(), 2) if len(rework_cost) > 0 else 0,
        'nombre_goulots_identifies': int(wait_bottlenecks['is_bottleneck'].sum()),
        'nombre_points_accumulation_wip': len(accumulation)
    }

    print("\nðŸŽ¯ KPIs Globaux:")
    for key, value in kpis.items():
        print(f"  â€¢ {key}: {value}")

    # Sauvegarder les rÃ©sultats
    output_dir = Path("outputs/reports")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Sauvegarder les KPIs
    with open(output_dir / "kpis_summary.json", "w") as f:
        json.dump(kpis, f, indent=2)

    # Sauvegarder les analyses dÃ©taillÃ©es
    wait_bottlenecks.to_csv(output_dir / "bottlenecks_wait_time.csv", index=False)
    wip_by_activity.to_csv(output_dir / "wip_by_activity.csv", index=False)
    rework_rate.to_csv(output_dir / "rework_rate.csv", index=False)
    cycle_times.to_csv(output_dir / "cycle_times.csv")

    print(f"\nðŸ’¾ RÃ©sultats sauvegardÃ©s dans: {output_dir}")

    print("\n" + "=" * 80)
    print("âœ… ANALYSE COMPLÃˆTE TERMINÃ‰E")
    print("=" * 80)

    return {
        'overview': overview,
        'wait_bottlenecks': wait_bottlenecks,
        'wip_by_activity': wip_by_activity,
        'rework_rate': rework_rate,
        'kpis': kpis
    }


if __name__ == "__main__":
    event_log_path = "data/event_logs/manufacturing_event_log.csv"
    results = run_complete_analysis(event_log_path)
